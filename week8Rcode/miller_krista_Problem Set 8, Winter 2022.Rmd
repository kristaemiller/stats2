---
title: "Problem Set 8, Winter 2022"
author: "Krista Miller"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=TRUE}

# Load any packages, if any, that you use as part of your answers here
# For example: 

library(MASS)
library(glmnet)
library(mlbench)
library(survival)
library(survminer)

```

CCONTEXT - HOUSE VALUES IN BOSTON, CIRCA 1970

This dataset was obtained through the mlbench package, which contains a subset of data sets available through the UCI Machine Learning Repository. From the help file:

Housing data for 506 census tracts of Boston from the 1970 census. The dataframe BostonHousing contains the original data by Harrison and Rubinfeld (1979).

The original data are 506 observations on 14 variables, medv being the target variable:

Continuous variables:

crim	      per capita crime rate by town 
zn       	proportion of residential land zoned for lots over 25,000 sq.ft  
indus   	   proportion of non-retail business acres per town
nox	      nitric oxides concentration (parts per 10 million)
rm	         average number of rooms per dwelling
age	      proportion of owner-occupied units built prior to 1940
dis	      weighted distances to five Boston employment centres
rad	      index of accessibility to radial highways
tax	      full-value property-tax rate per USD 10,000
ptratio	   pupil-teacher ratio by town
b	         1000(B - 0.63)^2 where B is the proportion of blacks by town
lstat	      percentage of lower status of the population
medv	      median value of owner-occupied homes in USD 1000's

Categorical variables: 

chas	    Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)

## Question 1 - 10 points

First, load the data into memory. The variable types are already stored in this data set. 
```{r}

data(BostonHousing) # loads the BostonHousing dataset into memory from the mlbench package

str(BostonHousing)

```

Before you begin your analysis, you will split the data into a 70% training set and a 30% test set. First, save the number of rows in the data set for use in the splitting code.

```{r}

n <- nrow(BostonHousing) 

```

When splitting data into training/test data sets, it's good practice to set a random seed to create a split that's reproducible. For this question, use the following seed.

```{r}

set.seed(123456) 

```

In Problem Set 6, you were shown some code from the async to create a train-validate-test split

tvt2 <- sample(rep(0:2,c(round(n*.2),round(n*.2),n-2*round(n*.2))),n)

In this problem, however, you are splitting your data into just training and test sets (i.e., just two groups). You can make some changes to the rep() function contained in this line code to create a split for just train-test. To help you make these adaptations, the following code chunk contains the isolated version of what's contained in the tvt2 rep() function. Run it to see what it produces and then make alterations that will instead produce a set of 0's (test set, 30%) and 1's (training set, 70%) for splitting purposes.

```{r}

tvt2.rep <- rep(0:1,c(round(n*.3),round(n*.7))) # The .2 in this function produces a 80% train/20% validation/20% test split in the data

tvt2.rep # Shows the result in the console window

table(tvt2.rep) # Shows a count of the 0's (test), 1's (valid), and 2's (train)

# Here is some room for you to change things and test how they work

```

Once you've found something that works, insert it into the blank space in tv.split to obtain a 70% training/30% test split. Display a table of the split to verify that approximately 70% of tv.split is equal to 1 and approximately 30% is equal to zero
```{r}

set.seed(123456) # Be sure to re-run this line right before running the following line

tv.split <- sample(rep(tvt2.rep),n)
   
table(tv.split)   

dat.train <- BostonHousing[tv.split==1,] 
dat.test <- BostonHousing[tv.split==0,] 

```

## Question 2 - 10 points 

After completing Question 1, conduct a cross-validated ridge regression using the training data set. Use medv as the outcome and all of the other variables in the data set as the predictors. 

```{r}

# Your code to get the training data into the proper form to conduct cross-validated ridge regression

x<- model.matrix(medv~., dat.train)
x<- x[,-1]
y<- dat.train$medv

# Your code to conduct cross-validated ridge regression

set.seed(123456)

cvfit.house.ridge <- cv.glmnet(x, y, alpha=0) #alpha=0 for ridge regression

```

For this question, the only lambda of interest is lambda.min. Make sure that lambda.min and the coefficients associated with it are visible in your knitted document. 

```{r}
# Display your lambda.min here
cvfit.house.ridge$lambda.min

# Display the coefficients associated with lambda.min here
coef(cvfit.house.ridge, s = "lambda.min")

```

## Question 3 - 5 points

Using the results from Question 2, compute the mean squared prediction error for the lambda.min model when applied to the *test* data set. Be sure to show how you computed it and to display the result; once you've done that, answer the question below.

```{r}

# Your code to get the test data into the proper form to compute predicted values using the coefficients associated with the lambda.min as fitted on the training set
xtest<- model.matrix(medv~., dat.test)
xtest<- xtest[,-1]

# Your code to obtain the mean squared prediction error
pred<- predict(cvfit.house.ridge, xtest, c(cvfit.house.ridge$lambda))


MSPE <-apply(pred,2,function(x){mean((x-dat.test$medv)^2)})
MSPE
best<-which(MSPE==min(MSPE))
best

cvfit.house.ridge$lambda[best]
```

A) What is the mean squared prediction error you computed (your answer here): 

0.68078

CONTEXT - NYC BIKERS

The NYC Open Data Portal contains information about the number of cyclists who cross different bridges in the eastern part of New York City. The data for this question is an edited subset of the data available. To see the full data, see https://data.cityofnewyork.us/Transportation/Bicycle-Counts-for-East-River-Bridges/gua4-p9wg. 

Variables of interest for this question (all are continuous):

M_bridge_count: The daily count of cyclists who ride across the Manhattan Bridge
temp_hi: The highest temperature recorded that day (in Fahrenheit)
precipitation: The amount of precipitation recorded that day (in inches)

## Question 4 - 15 points

The outcome of interest in this analysis is M_bridge_count. If you look at the values in this variable, you will see that the values contain commas to mark the thousandths place. First, remove the commas using any method, then demonstrate that the commas have been removed by displaying the first few values of the cleaned variable the head() function

```{r}

bike<-read.csv("NYCBikes.csv")

# Code to remove the commas in M_bridge_count goes here
bike$M_bridge_count <- as.numeric(gsub(",","",bike$M_bridge_count))

# Use the head() function to show in your knitted document that the commas have been removed. If you saved the cleaned variable into a new variable, replace M_bridge_count with the name of your cleaned variable

head(bike$M_bridge_count)

```

Your two predictor variables, temp_hi and precipitation, should already be numeric. Your cleaned outcome variable, however, may need to be re-typed as a numeric variable. 

```{r}

# Code to re-type your cleaned M_bridge_count variable

bike$M_bridge_count <- as.numeric(bike$M_bridge_count)

# Display with the str function to verify that variables are correctly typed

str(bike)

```

Now you will fit three models using this data: a Poisson model, a quasipossion model, and a negative binomial model. The outcome of these analyses should be M_bridge_count, and the predictors should be temp_hi and precipitation. 

Poisson model
```{r}

model.poisson <- glm(M_bridge_count~ temp_hi + precipitation, data= bike, family= "poisson")

summary(model.poisson)

```

Quasipoisson model
```{r}

model.quasipoisson <- glm(M_bridge_count~ temp_hi + precipitation, data= bike, family= "quasipoisson")

summary(model.quasipoisson)

```

Negative binomial model
```{r}

model.nb <- glm.nb(M_bridge_count~ temp_hi + precipitation, data= bike)

summary(model.nb)

```

Once you've fit all three models, answer the three questions below.

A) Look at the output for the Poisson model and the quasipoisson model. Which of these - Poisson or quasipoisson - have larger standard errors for the coefficients?

Your answer here (Poisson or quasipoisson): quasipoisson

B) Look at the quasipoisson model output. What was the dispersion parameter taken to be in your model?

Your answer here: 352.1449

C) Per the guidelines presented in the async and discussed during the live session, which of the three models - Poisson, quasipoisson, and negative binomial - is the best based on the *residual deviance*?

Your answer here (Poisson, quasipoisson, or negative binomial): negative binomial


## Question 5 - 15 points

Before beginning this question, please review the material from 9.4.3 in the async material. 

The following code is excerpted from the example shown in 9.4.3. The outcome of interest is time to death of sheep. Each sheep received some level of anti-parasite treatment; A and B contained actual anti-parasite ingredients and C was a placebo (i.e., no active ingredient in the treatment). Please run the three code chunks and examine their output. Once you've done that, answer the four questions below.

```{r}

# Chunk 1

sheep<-read.csv("sheep.deaths.csv")

with(sheep,plot(survfit(Surv(death,status)~group),lty=c(1,3,5),xlab="Age at Death (months)"))
legend("topright", c("A", "B","C"), lty = c(1,3,5))


```

```{r}

# Chunk 2

model<-survreg(Surv(death,status)~group, dist="exponential",data=sheep)
summary(model)
```

```{r}

# Chunk 3

plot(survfit(Surv(sheep$death,sheep$status)~sheep$group),lty=c(1,3,5),xlab="Age at Death (months)")
legend("topright", c("A", "B","C"), lty = c(1,3,5))

points(1:50,
       1-pexp(1:50,rate=1/exp(model$coefficients[1])),
       type="l",
      lty=1)
# The survival curve S(t) for group B.
points(1:50,
       1-pexp(1:50,rate=1/exp(sum(model$coefficients[c(1,2)]))),
       type="l",
      lty=3)
# The survival curve S(t) for group C.
points(1:50,
1-pexp(1:50,rate=1/exp(sum(model$coefficients[c(1,3)]))),
       type="l",
      lty=5)


```

# Question about Chunk 1

A) What kind of plot is this? It has a specific name.

Your answer here: Kaplan-Meier curve

# Questions about Chunk 2

B) What kind of survival model is being fitted in this code?

Your answer here: exponential

C) What does the output of the model fitted using the survreg() function suggest about the treatment groups (A, B, and C)?

Your answer here: 
Belonging to groupA (treatment group) has a longer survival time than group B or C. 
Belonging to groupB accelerates the time to event by a factor of exp(-0.671) = (0.51 times shorter survival time compared to the baseline survival).
Belonging to groupC accelerates the time to event by a factor of exp(-1.386) = (0.25 times shorter survival time compared to the baseline survival). 


# Question about Chunk 3

D) The jagged lines on this plot are the same as those from the plot shown in Chunk 1. What is being visualized by the the *smooth, curved lines* in this plot?

Your answer here: Goodness of fit of the survival model. 


