---
title: "COMP 4442 Final Exam, Winter 2022"
author: "Krista Miller"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)

library(glmnet)
library(MASS)
library(survival)
library(survminer)
library(dplyr)

# Load any additional packages, if any, that you use as part of your answers here


```

There are four questions on this midterm, all of which have multiple parts. Please be sure to provide answers to all parts of each question. Each question has an associated .csv file, which you will load into memory at the beginning of the question. All of the included data sets are simulated, so any results should not be taken as evidence for or against the existence of anything in the real world. The data were simulated to minimize the ambiguity and messiness that typifies real data. If you feel that something is ambiguous in a way that impedes your ability to answer the questions, please let me know. 

Almost there - you can do it! 

## Question 1 - Train/validate/test (30 points total)

For this question, a 400 observation data set was split such that 50% of the data was assigned to a training set, 25% was assigned to a validation set, and the remaining 25% was assigned to a test set. The outcome variable is y, which is contained in all three data sets, and is continuous. The predictor variables, also contained in all three data sets, are all continuous.

Run the code chunk below to load the data into memory before beginning your work on this question. 

```{r}

q1.train <- read.csv("q1_train.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer

q1.valid <- read.csv("q1_valid.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer

q1.test <- read.csv("q1_test.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer

# Check variable types 

str(q1.train)
str(q1.valid)
str(q1.test)

```

# Q1, Part 1 - Generating candidate models on the training set (15 points)

You will generate four candidate models: a model obtained through forward selection, a model obtained through backward selection, a cross-validated ridge regression model, and a cross-validated lasso regression model. For both cross-validated models, use the model associated with lambda.1se. You will fit all of these models to the training data (q1.train), using y as the outcome and x1-x9 as the predictors. Please do not include interactions or squares in your pool of potential predictors. Once you have done this, answer the four questions below.

Forward selection: 
```{r}

# Code to set up your training data for forward selection
glm.null<- glm(y~1, data= q1.train)
all<-glm(y~., data= q1.train)

# Code to conduct forward selection - be sure to include trace=1

model.forward <- step(glm.null, direction= 'forward', scope=formula(all), trace=1)

# Display the final model selected by forward selection here

summary(model.forward)
  
```

Backward selection: 
```{r}

# Code to set up your training data for backward selection
glm.null<- glm(y~1, data= q1.train)
all<-glm(y~., data= q1.train)

# Code to conduct backward selection - be sure to include trace=1

model.backward <- step(all, direction= 'backward',  trace=1)

# Display the final model selected by backward selection here

summary(model.backward)

```

Cross-validated ridge regression (lambda.1se); please use the provided random seed
```{r}

# Code to set up your training data for cross-validated ridge regression

x<- model.matrix(y~x1+x2+x3+x4+x5+x6+x7+x8+x9, q1.train)
x<- x[,-1]
y<- q1.train$y

set.seed(123456)

# Code to conduct cross-validated ridge regression here

cv.ridge <- cv.glmnet(x, y, alpha=0) #alpha=0 for ridge regression

# Display lambda.1se
cv.ridge$lambda.1se
  
# Display the coefficients associated with lambda.1se 
coef(cv.ridge, s= "lambda.1se")

```

Cross-validated lasso regression (lambda.1se); please use the provided random seed
```{r}

# Code to set up your training data for cross-validated lasso regression
x2<- model.matrix(y~x1+x2+x3+x4+x5+x6+x7+x8+x9, q1.train)
x2<- x2[,-1]
y2<- q1.train$y

set.seed(123456)

# Code to conduct cross-validated lasso regression here

cv.lasso <- cv.glmnet(x2, y2, alpha=1) #alpha=1 for lasso regression

# Display lambda.1se
cv.lasso$lambda.1se
  
# Display the coefficients associated with lambda.1se 
coef(cv.lasso, s= "lambda.1se")

```

A) What predictors are included in the model selected by forward selection?

Your answer here: x7, x2, x6, x9

B) What predictors are included in the model selected by backward selection?

Your answer here: x2, x6, x7, x9

C) What predictors are included in the cross-validated ridge model (lambda.1se)?

Your answer here: x1, x2, x3, x4, x5, x6, x7, x8, x9

D) What predictors are included in the cross-validated lasso model (lambda.1se)?

Your answer here: x2, x6, x7


# Q1, Part 2 - Fitting candidate models on the validation set and choosing a model (10 points)

Now, use the validation data (q1.valid) to generate new predictions for the four candidate models obtained in the previous part. Use these to compute the mean squared prediction error to evaluate how well each candidate model (which was estimated using training data) predicts the y's in the validation data set. Once you have done this, answer the three questions below.

Forward model:
```{r}

# Obtain new predictions using the validation set

cvpred.forward<- predict(model.forward, q1.valid, type="response")

# Compute mean squared prediction error for the forward model

mspe.forward<- mean((q1.valid$y-cvpred.forward)^2)
mspe.forward

```

Backward model:
```{r}

# Obtain new predictions using the validation set

cvpred.backward<- predict(model.backward, q1.valid, type="response")

# Compute mean squared prediction error for the backward model

mspe.backward.valid <- mean((q1.valid$y-cvpred.backward)^2)
mspe.backward.valid 

```

Cross-validated ridge model (lambda.1se):
```{r}

# Obtain new predictions using the validation set

ridge.valid<- model.matrix(y~x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9, q1.valid)
ridge.valid<- ridge.valid[,-1]

ridge.valid.pred<- predict(cv.ridge, ridge.valid)

# Compute mean squared prediction error for the cross-validated ridge model

mspe.ridge.valid <- mean((q1.valid$y-ridge.valid.pred)^2)
mspe.ridge.valid 

```

Cross-validated lasso model (lambda.1se):
```{r}

# Obtain new predictions using the validation set
lasso.valid<- model.matrix(y~x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9, q1.valid)
lasso.valid<- lasso.valid[,-1]

lasso.valid.pred<- predict(cv.lasso, lasso.valid)

# Compute mean squared prediction error for the cross-validated lasso model

mspe.lasso.valid <- mean((q1.valid$y-lasso.valid.pred)^2)
mspe.lasso.valid 

```

A) List the mean squared prediction error for each of your candidate models here:

Forward model mean squared prediction error: 24.01
Backward model mean squared prediction error: 24.01
Ridge model mean squared prediction error: 26.42
Lasso model mean squared prediction error: 27.47

B) Based on the mean squared prediction error, which model do you choose? 

Your answer here: forward

C) Display the coefficient values for the model you chose here:

```{r}

# Display candidate model chosen based on performance on the validation set here
summary(model.forward)


```


# Q1, Part 3 - Testing the final model on the test set (5 points)

Now that you've chosen a model, you will test this model's generalizability by generating new predictions for the final model chosen in the previous part. Use these new predictions to compute the mean squared prediction error to evaluate how well the final model (which was estimated using training data) predicts the y's in the test data set. Once you've done this, answer the question below:

```{r}

# Obtain new predictions using the test set

cvpred.final<- predict(model.forward, q1.test, type="response")

# Compute mean squared prediction error for the final model

mspe.final <- mean((q1.test$y-cvpred.forward)^2)
mspe.final

```

A) What is the mean squared prediction error for the final model making predictions on the test set?

Your answer here: 62.30




## Question 2 - Evaluating the generalizability of a "shrunk" logistic regression model (30 points total)

For this question, a different 400-observation data set was split such that 70% of it was assigned to a training set and the remaining 30% was assigned to a test set. The outcome variable is y, which is binary. The predictor variables, V1 - V10, are all continuous. 

Run the code chunk below to load the data into memory before beginning your work on this question.

```{r}

log.train <- read.csv("logistic_train.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer

log.test <- read.csv("logistic_test.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer

str(log.train) # Note: y is an int type; don't change this for this question
str(log.test) # Note: y is an int type; don't change this for this question

```

# Q2, Part 1 - Fitting the "shrunk" logistic model (5 points)

I fitted a penalized logistic regression model to the training data; that is, I applied a method of coefficient shrinkage to a logistic regression model fitted to the training. Run the code chunk below to fit this model and answer the two questions below.

```{r}

x.train.mat<-model.matrix(y~.,data=log.train)
x.train.matrix<-x.train.mat[,-1] 

y.train.vec<-log.train$y

set.seed(12345)

cvfit.log = cv.glmnet(x=x.train.matrix, y=y.train.vec, family="binomial", alpha=1)  
coef(cvfit.log, s = "lambda.min")

```

A) What type of penalized/"shrunk" logistic regression model did I fit to the training data? Be specific. 

Your answer here: cross-validated lasso regression

B) Which predictors (not including the intercept) are in the lambda.min penalized/"shrunk" logistic regression model? 

Your answer here (list the included predictors): V1, V4, V7, V8, V10


# Q2, Part 2 - Obtaining new predictions from test data (5 points)

In this section, you will generate new predictions of Y based on the values of the predictors in the test set. That is, you will use the predictor values from log.test to predict new y values. 

```{r}
x.test.mat<-model.matrix(y~.,data=log.test)
x.test.matrix<-x.test.mat[,-1] 

probs<-cvfit.log %>% predict(newx = x.test.matrix, type= "response")
y.test.pred <- ifelse(probs > 0.5, 1, 0)

```

Next, display the counts of both the predicted y values and the actual y values in log.test. Remember that these are binary variables. Hint: this will help you check your work in the next part. 

```{r}

# Counts for predicted y's

table(y.test.pred)

# Counts for actual y's

table(log.test$y)

```


# Q2, Part 3 - Evaluating the generalizability on the test set (20 points)

For this part, you will create a confusion matrix and compute the four model indices we discussed in class: accuracy, precision, recall, and F1 score. Use the table() function to create the confusion matrix and be sure to display it in the knitted document. Write your code for this below and answer the two questions below.

```{r}

## Your code for creating and displaying the confusion matrix

confusion.matrix <- table(Actual = log.test$y, Predicted = y.test.pred)
confusion.matrix

```

```{r}

## Your code for computing and displaying the model indices 

# Code to compute accuracy

accuracy <- sum(diag(confusion.matrix))/sum(confusion.matrix)
accuracy

# Code to compute precision

precision <- confusion.matrix[2,2]/sum(confusion.matrix[,2])
precision

# Code to compute recall

recall <- confusion.matrix[2,2]/sum(confusion.matrix[2,])
recall

# Code to compute F1 score

F1 = 2*(precision*recall)/(precision+recall)
F1

```

A) Fill in the values of your confusion matrix. Note that you should also have the confusion matrix displayed as part of the output from your code. 

    True positives (your answer here): 42
    False positives (your answer here): 23
    True negatives (your answer here): 24
    False negatives (your answer here): 11

B) Fill in the values of your model indices. Again, be sure that you also have these displayed as part of the output from your code. 

    Accuracy: 0.66
    Precision: 0.64
    Recall: 0.79
    F1 score: 0.71



## Question 3 - Modeling count data (20 points total)

Alyssa analyzed workplace injury incidents obtained from a shipping warehouse. She sampled 45 employee records at random and recorded the number of workplace injury reports in the employee's file in the past five years ("reports") and how many years since the employee underwent safety training certification ("years"). 

Run the code chunk below to load the data into memory before beginning your work on this question. 

```{r}

injury <- read.csv("injury.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer

str(injury)

```

# Q3, Part 1 - Describing the data (10 points)

Before starting the analysis, Alyssa conducted some descriptive analyses to present alongside her modeling work. Please recreate the descriptive analyses she conducted by answering the following three questions. Any code you use to answer the questions should go into the chunk below:

```{r}
# Code for your descriptive analyses here
#percentage of employees with zero or one injury in their file:
zero_or_one_injuries<- as.table(table(injury$reports<2))
prop.table(zero_or_one_injuries)

#among workers who had 6 or more injury reports, what was the median number of years since their most recent safety certification?
six_or_more_injuries<- filter(injury, reports>5)
median(six_or_more_injuries$years)

#among workers who had a safety certification that was less than 5 years old, what was the median number of injury reports?
safety_less_than_5_yrs<- filter(injury, years<5)
median(safety_less_than_5_yrs$reports)

```

A) What percentage of the sampled employees had zero or one injury report in their files? 

Your answer here: 51.1%

B) Among workers who had 6 or more injury reports, what was the median number of years since their most recent safety certification?

Your answer here: 8.5 years

C) Among workers who had a safety certification that was less than 5 years old, what was the median number of injury reports?

Your answer here: 1


# Q3, Part 2 - Fitting a quasipoisson model (10 points)

After some exploratory modeling, Alyssa determined that a *quasipoisson* model was the most appropriate model for these data. 

First, please re-create her analysis by fitting a quasipoisson model to the data, using reports as the outcome and years as the predictor. 

```{r}

# Code for your model here

model.quasipoisson <- glm(reports~ years, data= injury, family= "quasipoisson")
summary(model.quasipoisson)

```

Next, answer the following three questions about the analysis:

A) Was the number of years since safety training a significant predictor of the number of injury reports?

Your answer here (yes/no): yes

B) Based on the results, does the predicted count of workplace injury reports *increase* or *decrease* as the number of years since an employees most recent safety certification increases? How do you know? 

Your answer here: increase

C) Based on your answer to the previous question, do you think Alyssa recommended greater frequency of safety certification or a lower frequency of safety certification?

Your answer here (higher or lower): higher



## Question 4 - Modeling time-to-event data (20 points total)

As part of receiving services at a community mental health clinic, a prospective client must first submit a completed intake form; upon approval, the client may then schedule a first visit with a provider. Ahmed designed an intervention study to determine if different follow-up schedules ("group") decrease the amount of time in between the client's approval and the client's first appointment ("first_visit", in decimal days). Clients in Group A, which was the current standard for client follow-up at the time, received a reminder via text message to schedule their first appointment 14 days after approval. Clients in Group B received reminders 14 and 21 days after approval, and clients in Group C received reminders 7 and 14 days after approval. If the client saw a provider within 30 days of approval, "status" is equal to 1; otherwise, it's equal to zero. 

Run the code chunk below to load the data into memory before beginning your work on this question. 

```{r}

intake <- read.csv("enrollment.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer

str(intake)

```

# Q4, Part 1 - Kaplan-Meier curves and censoring (10 points)

Ahmed started his analysis by fitting Kaplan-Meier curves for each group, which can be seen by running the code chunk below. Please examine this visualization and answer the three questions below. 

```{r}

fit <- survfit(Surv(first_visit,status) ~ group,
               data = intake)
ggsurvplot(fit)

```

A) Based on what you see in the Kaplan-Meier curves, which group had the most censored observations?

Your answer here: Group A

B) The line for group C ends somewhere between 20 and 30 days. What does this mean for members of group C in terms of their time-to-first visit?

Your answer here: In group C, there is a 0% probability of event (time-to-visit) happening beyond approximately 26 days.  In other words, everyone in group C experienced the scheduled first visit event within approximately 26 days. 

C) During the design phase, Ahmed took steps to prevent left censoring from occurring. If this had happened and a client's status was left-censored, what would that have meant with regard to their time-to-first visit? 

Your answer here: The patient scheduled a first visit (event) with a provider prior to the intervention (follow up reminders).


# Q4, Part 2: Fitting parametric survival models (10 points)

Next, Ahmed fitted two accelerated failure time (AFT) models: an exponential AFT model and a Weibull AFT model. Run the code chunk below and review the results of the two analyses.

```{r}

model.weib<-survreg(Surv(first_visit,status)~group, dist="weibull",data=intake)
summary(model.weib)


model.exp<-survreg(Surv(first_visit,status)~group, dist="exponential",data=intake)
summary(model.exp)

```

He then created a Kaplan-Meier plot with overlaid cumulative distribution curves for both models. The orange cumulative distribution curves are derived from the fitted Weibull AFT model, and the blue curves are from the fitted exponential AFT model. Run the code chunk below to view these and answer the four questions below. 

```{r}

plot(survfit(Surv(first_visit,status) ~ group,
               data = intake),lty=c(1,3,5),xlab="Survival Probability")
legend("bottomleft", c("a", "b","c"), lty = c(1,3,5))
points(seq(0,30,by=.2),
       1-psurvreg(seq(0,30,by=.2),mean=model.weib$coefficients[1],
                  scale=model.weib$scale),type="l",lty=1,col="orange")
points(seq(0,30,by=.2),
       1-pexp(seq(0,30,by=.2),rate=1/exp(model.exp$coefficients[1])),
       type="l", lty=1,col="blue")
points(seq(0,30,by=.2),
       1-psurvreg(seq(0,30,by=.2),mean=sum(model.weib$coefficients[c(1,2)]),
                  scale=model.weib$scale),type="l",lty=3,col="orange")
points(seq(0,30,by=.2),
       1-psurvreg(seq(0,30,by=.2),mean=sum(model.exp$coefficients[c(1,2)]),
                  distribution = "exponential"),type="l",lty=3,col="blue")
points(seq(0,30,by=.2),
      1-psurvreg(seq(0,30,by=.2),mean=sum(model.weib$coefficients[c(1,3)]),scale=model.weib$scale),
       type="l",lty=5,col="orange")
points(seq(0,30,by=.2),
       1-psurvreg(seq(0,30,by=.2),mean=sum(model.exp$coefficients[c(1,3)]),
                  distribution = "exponential"),type="l",lty=5,col="blue")

```

A) Based on the visualization, which of the two models - exponential AFT (blue) or Weibull AFT (orange) - appear to best model the time-to-first visit for the different intervention groups?

Your answer here (exponential or Weibull): Weibull AFT

B) Look at the model output for the model you chose in the previous question. Is there evidence of a difference between Group A and Group B?

Your answer here (yes/no): yes 

C) From this same model output, is there evidence of a difference between Group A and Group C?

Your answer here (yes/no): yes

D) Considering the results shown in both parts 1 and 2 of this question, which follow-up schedule - Day 14 only, Days 7 and 14, or Days 14 and 21 - do you think Ahmed recommended to the clinic? Please note that stating the group letter alone here won't earn full credit; you must explicitly reference the specific intervention the group experienced. 

Your answer here: Ahmed likely recommended the follow up schedule that Group C received (text messages on days 7 and 14); belonging to group C accelerates the time to event by a factor of exp(-0.7144) 


## All done - you did it! Treat yourself to something nice :)